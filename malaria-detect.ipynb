{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for dirname, _, filenames in os.walk('/kaggle/input'):\n   # for filename in filenames:\n       # print(os.path.join(dirname, filename))\ndir = \"/kaggle/input/cell-images-for-detecting-malaria/cell_images/cell_images\"\nprint(os.listdir(dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = transforms.Compose([transforms.Resize((120, 120)),\n                                       transforms.ColorJitter(0.05),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.RandomVerticalFlip(),\n                                       transforms.RandomRotation(20),\n                                       transforms.ToTensor(), \n                                       transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n                                     ])\n\ntrain_set = datasets.ImageFolder(dir, transform = train_transforms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = 0.25\nnum_train = len(train_set)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\n\ntest_split = int(np.floor((test_size)*num_train))\ntrain_index, test_index = indices[test_split - 1 :], indices[: test_split - 1]\n\ntrain_sampler = SubsetRandomSampler(train_index)\ntest_sampler = SubsetRandomSampler(test_index)\n\ntrain_loader = DataLoader(train_set, sampler=train_sampler, batch_size=104)\ntest_loader = DataLoader(train_set, sampler=test_sampler, batch_size=58)\n\nprint(\"Images in Test set: {}\\nImages in Train set: {}\".format(len(test_index), len(train_index)))               ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes=['infected','uninfected']\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    \nimages, labels = next(iter(train_loader))\n\nfig = plt.figure(figsize=(25, 15))\n\nfor i in range(10):\n    ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[], title=classes[labels[i]])\n    imshow(images[i])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n            \n        self.fc1 = nn.Linear(64*15*15, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 2)\n        self.drop = nn.Dropout2d(0.2)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.size(0), -1)    # flatten out a input for Dense Layer\n        out = self.fc1(out)\n        out = F.relu(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = F.relu(out)\n        out = self.drop(out)\n        out = self.fc3(out)\n        \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NeuralNet()\nmodel.to(device)\nerror = nn.CrossEntropyLoss()\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\nbatch_size = 100 \n\nfor epoch in range(num_epochs):\n    train_loss = 0.\n    model.train()    # explictily stating the training\n    \n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        train = images.view(-1, 3, 120, 120)\n        outputs = model(train)\n        \n        optimizer.zero_grad()\n        loss = error(outputs, labels)\n        loss.backward()    #back-propagation\n        optimizer.step()\n        \n        train_loss += loss.item() * batch_size\n     \n    print(\"Epoch: {}, Loss: {:.4f}\".format(epoch + 1, train_loss / len(train_loader.dataset)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = 0\ntotal = 0\nclass_total = [0 for _ in range(2)]\nclass_correct = [0 for _ in range(2)]\nbatch_size = 20\n# Lists used in Confusion Matrix\nactual = []\npredict = []\n\nmodel.eval()    # explicitly stating the testing \nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to (device)\n        \n        actual.append(labels.data.tolist())\n        test = images.view(-1, 3, 120, 120)\n        outputs = model(test)\n        predicted = torch.max(outputs, 1)[1]\n        predict.append(predicted.data.tolist())\n        total += len(labels)\n        correct += (predicted == labels).sum().item()\n        # Calculating classwise accuracy\n        c = (predicted == labels).squeeze()\n        for i in range(batch_size):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n        \nprint(\"Accuracy on the Test set: {:.2f}%\".format(correct * 100 / total))\nprint()\nfor i in range(2):\n    print(\"Accuracy of {} :  {:.2f}%   [{} / {}]\".format(classes[i], class_correct[i] * 100 / class_total[i], \n                                           class_correct[i], class_total[i]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport itertools\n\n#flatten out 2D list into 1D\nactual = list(itertools.chain.from_iterable(actual))\npredict = list(itertools.chain.from_iterable(predict))\nresults = confusion_matrix(actual, predict)\nprint(\"Accuracy Score: \")\nprint(\"{:.4f}\".format(accuracy_score(actual, predict)))\nprint()\nprint(\"Report: \")\nprint(classification_report(actual, predict))\nprint()\nprint(\"Confusion Matrix: \")\nprint(pd.DataFrame(results, columns=[\"Predicted No\", \"Predicted Yes\"], index=[\"Actual No\", \"Actual Yes\"]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.heatmap(results, cmap=\"magma\", annot=True, fmt=\"d\", cbar=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}